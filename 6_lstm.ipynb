{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat_v2(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294413 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "dz ikfe  g    aewxiirbcanviy collsusqotzvozmetjw cyipuhz yreeoe rjrcho rdmswgwtc\n",
      "golsidabegaedzoeqnmbhpzfnubg mpkj g lddidto t pte lteee na jpxkatydeijrlnooulk p\n",
      "v wcrppeorhkaxz stuaneveviagb ed  lfrloeq dem cssnamauv dl ntutui indido l bzrxi\n",
      "sapteipbjeso mn in onuql chcc  qcgeed rbo  yedoibshh  ilkiot wnempem qxfoezm qld\n",
      "dalovu ddwtreiqe dwvdhudw bbfip genuglipfgufvt ormewfkalxxl aiqiljgnwiez mgaa w \n",
      "================================================================================\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 100: 2.589133 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.01\n",
      "Validation set perplexity: 10.26\n",
      "Average loss at step 200: 2.239149 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.48\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 300: 2.101283 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 400: 2.005828 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 500: 1.943361 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 600: 1.914288 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 700: 1.865447 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 800: 1.829403 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 900: 1.836157 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.834247 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "beht for he speemeses incluoug and indany of the gove fium delich one feven of a\n",
      " sum anti the disters ryffan the sweef comisc s be ken kivole of stith its to wi\n",
      "res and three s origino of six sho in hapitury of erage far predated incopts bed\n",
      "caite comac and two one nine fram of he ductorda damcre ippusizer his and nearm \n",
      "us scinifreicy heme moys listins twuch at the mexe to rouns and ugidere exo the \n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1100: 1.783503 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1200: 1.759654 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1300: 1.742621 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1400: 1.752100 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1500: 1.744810 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1600: 1.751068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1700: 1.717589 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.678913 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1900: 1.653399 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2000: 1.705722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "tive croungingly propectives sevid and vidiorol one three nine five four four tw\n",
      "a mandhist my ase and bebort early to the ids bus pointary he glath extension cl\n",
      "x a natse play don schement contrictiturity is decarqutate chrap in see sut thou\n",
      "ciam gonon usidee the leade to vorjoble and inecting inda wirz his the experentu\n",
      "unced so his dononous ond worliasir the doringual sample of mirders unancoung tw\n",
      "================================================================================\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2100: 1.690399 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2200: 1.683365 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2300: 1.649539 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2400: 1.669149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2500: 1.687643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2600: 1.663902 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2700: 1.659408 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2800: 1.655579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2900: 1.655735 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3000: 1.654703 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "onsola do lock all to notrestically national not the tion theora ona beccue and \n",
      "rombho just one of the vicy is roud base and nwme are annical tramber that ue ca\n",
      "javer with and animating incoponingle fult liwe liverces was occasis since the o\n",
      "thalinc three spacent parland of discoan s thein settlen sole thium prack is suc\n",
      "urs that distrosices has solfored to expeativultionilan margeed the films jaoped\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3100: 1.632291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3200: 1.653205 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3300: 1.639214 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3400: 1.672163 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.664467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3600: 1.673034 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3700: 1.651827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3800: 1.648114 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3900: 1.638061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4000: 1.653108 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      " in the individal graces and give voammetic theory vinese is of the kerodoms are\n",
      "piring caputed a was medilate hews after extsom pribalsbadumar which mystrupe re\n",
      "riticon way havin transfouguage beaponry eight wiety links one yedvom re its to \n",
      "uts althove head to retedeted and b contanu physerian notion and his promon with\n",
      "red of the flowtoningure interperity one nine zero of the one one nine nine mith\n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4100: 1.635584 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4200: 1.640188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4300: 1.619162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4400: 1.613173 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4500: 1.617922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4600: 1.620827 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4700: 1.629667 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4800: 1.632400 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4900: 1.636643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5000: 1.611193 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "k hendil disteres his game codely and woid dondy basiders romb claters hunre sta\n",
      "jant the lotter dia a nittol breapry of at include boaks and slow the remused id\n",
      "p common mallity of the parys off a by one six for neor addistion r cummented to\n",
      "y denable times azeramial hame salenoulop capper bood bath shortally that in pat\n",
      "jentiel cent controls of the dupss of rathen dejupiby his the s in a cranse it p\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5100: 1.606107 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5200: 1.592268 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5300: 1.586296 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5400: 1.581691 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5500: 1.570891 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5600: 1.583971 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5700: 1.572030 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5800: 1.585260 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5900: 1.578386 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6000: 1.551142 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "t unceapances out six four new leamon in persociation of any frox the cavifie le\n",
      "polendanco placizeature the some tankan vione of emethens and it relision reigni\n",
      "har indata house statation dosic mater dovin of and might include the glago reve\n",
      "pet that deattism perpering the ecomentary proposon observents and manak unlosit\n",
      "helie the one nine six by funct of side sermidicantly the pritory two see gase t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6100: 1.564389 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6200: 1.536509 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6300: 1.545506 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6400: 1.546694 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6500: 1.559673 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6600: 1.599108 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6700: 1.584037 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6800: 1.605298 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6900: 1.587939 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 7000: 1.581747 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      " safe the doman of chebeleg dami the government the indeging appear to ring cair\n",
      "lement controllitou haddimary penomachy the man pred in retechnited to learis co\n",
      "ftex reguric cannat is wates for a suppt ad was very of the stituskning of ionsw\n",
      "gentinighe by barlsu seduazitips of failord the france ever other a cran in the \n",
      "phas than drims a fadabmerit face the recompedioried well movia thuder one nine \n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "# LSTM weights/biases, e.g. w_lstm_input, accounts for four of the previous variables, e.g. ix, fx, cx, ox.\n",
    "# LSTM cell (lstm_cell) now only has two matrix multiplications.\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # LSTM weights/biases\n",
    "  w_lstm_input = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  b_lstm = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  w_lstm_output = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    p = tf.matmul(i, w_lstm_input) + tf.matmul(o, w_lstm_output) + b_lstm\n",
    "    input_gate = tf.sigmoid(p[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(p[:, num_nodes:(2 * num_nodes)])\n",
    "    update = p[:, (2 * num_nodes):(3 * num_nodes)]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(p[:, (3 * num_nodes):])\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat_v2(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  keep_prob=tf.placeholder(tf.float32)\n",
    "  # Bigram embeddings\n",
    "  embed = tf.Variable(tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -0.1, 0.1))\n",
    "  # LSTM weights/biases\n",
    "  w_lstm_input = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  b_lstm = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  w_lstm_output = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"LSTM cell with fewer matrix multiplies.\"\"\"\n",
    "    p = tf.matmul(tf.nn.dropout(i, keep_prob), w_lstm_input) + tf.matmul(o, w_lstm_output) + b_lstm\n",
    "    input_gate = tf.sigmoid(p[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(p[:, num_nodes:(2 * num_nodes)])\n",
    "    update = p[:, (2 * num_nodes):(3 * num_nodes)]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(p[:, (3 * num_nodes):])\n",
    "    output = tf.nn.dropout(output_gate * tf.tanh(state), keep_prob)\n",
    "    return output, state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size, vocabulary_size]))\n",
    "  train_inputs = list()\n",
    "  for i in range(num_unrollings - 1):\n",
    "    train_inputs.append(tf.nn.embedding_lookup(embed, tf.argmax(train_data[i], 1) * vocabulary_size + tf.argmax(train_data[i + 1], 1)))\n",
    "  train_labels = train_data[2:]\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat_v2(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[2, vocabulary_size])\n",
    "  sample_input_embed = tf.expand_dims(tf.nn.embedding_lookup(embed, tf.argmax(sample_input[0], 0) * vocabulary_size + tf.argmax(sample_input[1], 0)), 0)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299389 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.10\n",
      "================================================================================\n",
      "flaeb qvaki aim lx  srr rjeuweqrregtkna jel   zscbhejutlztd e t  lzgni pdtacst qi\n",
      "mwimg ai tue  dolstgo  mxfhyp z mge oifv yeaikn ioyos srhhtsgimkhfbc ripumobm uqe\n",
      "jdrq efh ujg rpduehev szzirhhhto clixaisleknao lfc swvqi uy czhxs clegghhn  shg r\n",
      "lcyix r yer ych ferorihllibiu tetknnnxtclmgq oe mg o un   uj  sxagalu jy wyesgen \n",
      "hozeteycabn icez bzraqy be pavzwzdqmyhpcrvelr  ub dvjytabplmgqe unnwrhveimnhssjtn\n",
      "================================================================================\n",
      "Validation set perplexity: 20.16\n",
      "Average loss at step 100: 2.630300 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.70\n",
      "Validation set perplexity: 10.84\n",
      "Average loss at step 200: 2.343119 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.78\n",
      "Validation set perplexity: 9.28\n",
      "Average loss at step 300: 2.264440 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.94\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 400: 2.233430 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.41\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 500: 2.209664 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 8.37\n",
      "Average loss at step 600: 2.164192 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.73\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 700: 2.154075 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.51\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 800: 2.167077 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.71\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 900: 2.154081 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.36\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 1000: 2.169805 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.25\n",
      "================================================================================\n",
      "sqoebe ponas shing the won mosing morn speadeen tercation derfasy threp hory oxer\n",
      "tlu not the arimostliand ku rac stras patis one one naims doko exirgemal the gdiv\n",
      "e gusia three eme egemv one calle wave pecwart the sift frocois ame of trecdtimar\n",
      "dbor of eight as  the one nine nons ving auh anstive  is the redeapention ito e n\n",
      "ded seve and pand are upn on one nine for aiss hase aride helliall coab guyinniti\n",
      "================================================================================\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 1100: 2.125795 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.86\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 1200: 2.113196 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.07\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 1300: 2.113164 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.52\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 1400: 2.122804 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.40\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 1500: 2.121427 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1600: 2.102171 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.21\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1700: 2.096825 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 1800: 2.087842 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.81\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1900: 2.085858 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 2000: 2.082004 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "================================================================================\n",
      "mq bes kicbhy pled roneirsi three auge morcontrob cacii two ann ofnobil sacests o\n",
      "der ardny army of of the cany one swerl themecich five synrod sperxemfart furir a\n",
      "ll annmoctionten by eightteure pades kingeruke olactess on of bator deith fourmme\n",
      "qn contion mery quacoisms youttd four the strar coattenne in the lal amproclesto \n",
      "rbointerns and dafter of two five are one founfhvmustan aoto agentort wroled the \n",
      "================================================================================\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 2100: 2.088040 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 2200: 2.109250 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2300: 2.110766 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.50\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 2400: 2.102652 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.54\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 2500: 2.093728 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.13\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 2600: 2.090034 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 2700: 2.099710 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 2800: 2.095387 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 2900: 2.088117 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.11\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 3000: 2.092939 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "================================================================================\n",
      "nk the ik nor comst ter tween a meter user efferred eight zero the fecio u same s\n",
      "striber of grombe ertal the danlight ofne and fremarun temerpint ind freljsish th\n",
      "  an kcrughs ciding reas two runtic two four formic the and apsil sevent morted d\n",
      "tvure ferated fach three biper four repe an raction pil inlity eas the oternvers \n",
      "xz inves bet a as led seight mornan qoton isboturi and vex is themesse a of a cen\n",
      "================================================================================\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 3100: 2.074690 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 3200: 2.066533 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.27\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 3300: 2.072011 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.70\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3400: 2.064528 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.96\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3500: 2.101443 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.95\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 3600: 2.081621 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.50\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3700: 2.084448 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 3800: 2.086094 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.33\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 3900: 2.090027 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4000: 2.089751 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "================================================================================\n",
      "kkation the act the velsoundebut of aand one his vwing brousah whipeomith slent d\n",
      "aslation itar obat of shhenadmel redentemenefells and corbence atarepks conetath \n",
      "gong interphef one nine three the was on the cuwith gloction ciciet smlision nows\n",
      "gf modte mar wleon bdkhujavarhess ebasew nethen the sh ricade of beelopy vraintor\n",
      "rs ake wermeka the perphy invemecinge the is it is ducia nalso the withotion the \n",
      "================================================================================\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4100: 2.071041 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4200: 2.055495 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.14\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 4300: 2.069520 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 4400: 2.048821 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.18\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 4500: 2.086878 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.31\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 4600: 2.085200 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.73\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 4700: 2.077705 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.78\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4800: 2.084805 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.86\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 4900: 2.082740 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.98\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 5000: 2.074611 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.26\n",
      "================================================================================\n",
      "nvet pang a nald withue gillee nine the one nine idt seven the spenty of cotlesan\n",
      "ction surop bor an nelamgrectist resed itresiced precobilret the the neany hisp g\n",
      "tv ny to ind stresides anazeregx which and hat it liontanitiail two five coms but\n",
      "etic the in fromes alang wan river canes coxfial as as sombel bins four four incl\n",
      "t mestesbf pary fromally meraele from cor oration bolote the zero the the of a th\n",
      "================================================================================\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 5100: 2.052093 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.34\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 5200: 2.055587 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.42\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 5300: 2.058286 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.86\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 5400: 2.062938 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.23\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 5500: 2.062173 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.79\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 5600: 2.038324 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 5700: 2.034757 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5800: 2.048157 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 5900: 2.046684 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.32\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 6000: 2.038418 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.95\n",
      "================================================================================\n",
      "guoss in the ind throdoganen to sayh duvonimal stendon and owered comme in in cro\n",
      "rbenerd the zero as in wero one two six zero two mall of the madarany furitional \n",
      "ddine which shifnarloh beraltical also six two two nine nine the al as beer wor m\n",
      "gwrins the litadity in whist lisopon m sardectar of that muse dopularty klentenci\n",
      "pe free the cisudiele micculdy acersed mid at bretuaker anation k mf to krecacoxa\n",
      "================================================================================\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 6100: 2.031970 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 6200: 2.042236 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 6300: 2.043008 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.44\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 6400: 2.027497 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 6500: 2.013072 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 6600: 2.066901 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.24\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 6700: 2.039869 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.67\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 6800: 2.049328 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.03\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 6900: 2.030385 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 7000: 2.048531 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.01\n",
      "================================================================================\n",
      "e rak conmelu leary sush of to the greaad sine six the thuse suscon and repsonaa \n",
      "vtert me this and br was the of latiebrited bety duriciund in sernhomes one nat f\n",
      "skon nanre ins ther is rif three engupiend nolon to nve worctive nine chek the on\n",
      " has ocqyrication fersihocticals with pas morine maint and the to mos daysishikil\n",
      "wz kyilleconce arphy azagels mist adities is harallean pheasa rephint onnotz two \n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = {keep_prob: 0.5}\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = np.array([sample(random_distribution())[0] for _ in range(2)])\n",
    "          sentence = ''.join(characters(feed))\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed, keep_prob: 1.0})\n",
    "            feed = np.array([feed[1], sample(prediction)[0]])\n",
    "            sentence += characters(feed)[1]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      b = valid_batches.next()\n",
    "      for _ in range(valid_size):\n",
    "        predictions = sample_prediction.eval({sample_input: np.concatenate(b), keep_prob: 1.0})\n",
    "        b = valid_batches.next()\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sequence-to-sequence LSTM model which accepts a sequence of variable length (less that max_seq_len) and outputs the sequence in reverse order. We use sequences of digits instead of characters for simplicity, though it is just as general.\n",
    "\n",
    "This model features an encoder and decoder, each with their own parameters. The encoder encodes the input sequence and feeds its result to the decoder, which accepts the encoding and outputs the sequence in reverse order. In order to handle variable sequence lengths, the model uses padding and bucketing during its training phase.\n",
    "\n",
    "Some ways to improve this model:\n",
    "- Use tensorflow's built-in dynamic_rnn cells. I purposely avoided these to challenge myself to build the model from 'scratch.'\n",
    "- Use the 'attention' mechanism.\n",
    "- Use multi-layered LSTMs (computationally slower).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = 10\n",
    "max_seq_len = 5\n",
    "\n",
    "num_nodes = 32\n",
    "batch_size = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Encoder LSTM weights/biases.\n",
    "    w_enc = tf.Variable(tf.truncated_normal([vocab_size + 1 + num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    b_enc = tf.Variable(tf.constant(0.1, shape=[1, 4 * num_nodes]))\n",
    "    # Decoder LSTM weights/biases.\n",
    "    w_dec = tf.Variable(tf.truncated_normal([vocab_size + 1 + num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    b_dec = tf.Variable(tf.constant(0.1, shape=[1, 4 * num_nodes]))\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocab_size + 1], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocab_size + 1]))\n",
    "    \n",
    "    # Encoder LSTM.\n",
    "    def enc(i, output, state):\n",
    "        p = tf.matmul(tf.concat_v2([i, output], 1), w_enc) + b_enc\n",
    "        input_gate = tf.sigmoid(p[:, :num_nodes])\n",
    "        forget_gate = tf.sigmoid(p[:, num_nodes:(2 * num_nodes)])\n",
    "        update = p[:, (2 * num_nodes):(3 * num_nodes)]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(p[:, (3 * num_nodes):])\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n",
    "\n",
    "    # Decoder LSTM\n",
    "    def dec(i, output, state):\n",
    "        p = tf.matmul(tf.concat_v2([i, output], 1), w_dec) + b_dec\n",
    "        input_gate = tf.sigmoid(p[:, :num_nodes])\n",
    "        forget_gate = tf.sigmoid(p[:, num_nodes:(2 * num_nodes)])\n",
    "        update = p[:, (2 * num_nodes):(3 * num_nodes)]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(p[:, (3 * num_nodes):])\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n",
    "    \n",
    "    # Train data.\n",
    "    train_inputs = list()\n",
    "    train_labels = list()\n",
    "    for _ in range(max_seq_len):\n",
    "        train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size, vocab_size + 1]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocab_size + 1]))\n",
    "    train_eos = [tf.one_hot(tf.constant(vocab_size, shape=[batch_size]), vocab_size + 1)]\n",
    "\n",
    "    # Unrolled Encoder LSTM loop.\n",
    "    enc_output = tf.zeros([batch_size, num_nodes])\n",
    "    enc_state = tf.zeros([batch_size, num_nodes])\n",
    "    for i in train_inputs:\n",
    "        enc_output, enc_state = enc(i, enc_output, enc_state)\n",
    "    \n",
    "    # Unrolled Decoder LSTM loop.    \n",
    "    outputs = list()\n",
    "    dec_output = enc_output\n",
    "    dec_state = tf.zeros([batch_size, num_nodes])\n",
    "    for i in (train_eos + train_labels):\n",
    "        dec_output, dec_state = dec(i, dec_output, dec_state)\n",
    "        outputs.append(tf.matmul(dec_output, w) + b)\n",
    "\n",
    "    # Loss\n",
    "    logits = tf.concat_v2(outputs, 0)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat_v2(train_labels + train_eos, 0))) #+ 0.01 * (tf.nn.l2_loss(w_enc) + tf.nn.l2_loss(w_dec) + tf.nn.l2_loss(w))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.01, global_step, 5000 * max_seq_len, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Sampling.\n",
    "    sample_i = tf.placeholder(tf.float32, shape=[1, vocab_size + 1])\n",
    "    saved_sample_enc_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_enc_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_dec_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_dec_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample = tf.group(saved_sample_enc_output.assign(tf.zeros([1, num_nodes])),\n",
    "                            saved_sample_enc_state.assign(tf.zeros([1, num_nodes])), \n",
    "                            saved_sample_dec_output.assign(tf.zeros([1, num_nodes])),\n",
    "                            saved_sample_dec_state.assign(tf.zeros([1, num_nodes])))\n",
    "    init_dec = tf.group(saved_sample_dec_output.assign(saved_sample_enc_output))\n",
    "    \n",
    "    # Encode sample.\n",
    "    sample_enc_output, sample_enc_state = enc(sample_i, saved_sample_enc_output, saved_sample_enc_state)\n",
    "    train_enc = tf.group(saved_sample_enc_output.assign(sample_enc_output),\n",
    "                         saved_sample_enc_state.assign(sample_enc_state))\n",
    "    \n",
    "    # Decode sample.\n",
    "    sample_dec_output, sample_dec_state = dec(sample_i, saved_sample_dec_output, saved_sample_dec_state)\n",
    "    with tf.control_dependencies([saved_sample_dec_output.assign(sample_dec_output),\n",
    "                                  saved_sample_dec_state.assign(sample_dec_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.matmul(sample_dec_output, w) + b)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step: 0, Learning Rate: 0.010000, Average loss: 0.002358\n",
      "       Input | Output\n",
      "   [0 9 9 1] | []\n",
      " [7 6 0 9 2] | []\n",
      "       [8 7] | []\n",
      "         [5] | []\n",
      "   [0 1 1 5] | []\n",
      "Step: 1000, Learning Rate: 0.010000, Average loss: 0.190882\n",
      "       Input | Output\n",
      "         [7] | [7]\n",
      " [6 5 9 0 7] | [7 0 9 5 6]\n",
      "   [3 3 4 4] | [4 4 3 3]\n",
      " [2 4 2 8 9] | [9 8 2 4 2]\n",
      "         [5] | [5]\n",
      "Step: 2000, Learning Rate: 0.010000, Average loss: 0.011923\n",
      "       Input | Output\n",
      "   [1 5 6 6] | [6 6 5 1]\n",
      "         [0] | [0]\n",
      "       [5 6] | [6 5]\n",
      "       [3 3] | [3 3]\n",
      " [8 0 0 6 9] | [9 6 0 0 8]\n",
      "Step: 3000, Learning Rate: 0.010000, Average loss: 0.007677\n",
      "       Input | Output\n",
      " [9 9 8 9 2] | [2 9 8 9 9]\n",
      "   [9 1 2 7] | [7 2 1 9]\n",
      "     [5 3 4] | [4 3 5]\n",
      "       [5 3] | [3 5]\n",
      "     [8 6 4] | [4 6 8]\n",
      "Step: 4000, Learning Rate: 0.010000, Average loss: 0.005963\n",
      "       Input | Output\n",
      "         [8] | [8]\n",
      "         [3] | [3]\n",
      "   [7 0 0 0] | [0 0 0 7]\n",
      " [1 7 6 2 7] | [7 2 6 7 1]\n",
      "     [2 4 6] | [6 4 2]\n",
      "Step: 5000, Learning Rate: 0.001000, Average loss: 0.002227\n",
      "       Input | Output\n",
      "         [1] | [1]\n",
      " [9 0 2 4 0] | [0 4 2 0 9]\n",
      "   [5 0 4 7] | [7 4 0 5]\n",
      " [0 0 8 4 3] | [3 4 8 0 0]\n",
      " [0 9 5 3 5] | [5 3 5 9 0]\n",
      "Step: 6000, Learning Rate: 0.001000, Average loss: 0.000041\n",
      "       Input | Output\n",
      " [1 0 4 5 6] | [6 5 4 0 1]\n",
      "       [7 9] | [9 7]\n",
      " [5 3 8 9 3] | [3 9 8 3 5]\n",
      " [2 4 0 0 4] | [4 0 0 4 2]\n",
      " [2 3 8 6 1] | [1 6 8 3 2]\n",
      "Step: 7000, Learning Rate: 0.001000, Average loss: 0.000055\n",
      "       Input | Output\n",
      "   [5 1 9 2] | [2 9 1 5]\n",
      "         [2] | [2]\n",
      " [9 9 0 5 6] | [6 5 0 9 9]\n",
      "     [2 7 2] | [2 7 2]\n",
      "     [5 7 6] | [6 7 5]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        # At each step, train model with a batch for every sequence length.\n",
    "        for seq_len in range(1, max_seq_len + 1):\n",
    "            feed_dict = dict()\n",
    "            \n",
    "            # Random seq_len number of digits,\n",
    "            for i in range(seq_len):\n",
    "                feed = (np.arange(vocab_size + 1) == np.random.randint(vocab_size, size=batch_size)[:, None]).astype(np.float)\n",
    "                feed_dict[train_inputs[i]] = feed\n",
    "                feed_dict[train_labels[seq_len - 1 - i]] = feed\n",
    "                \n",
    "            # Padded with EOS.\n",
    "            for i in range(seq_len, max_seq_len):\n",
    "                feed_dict[train_inputs[i]] = (np.arange(vocab_size + 1) == np.full(batch_size, vocab_size, dtype=np.int64)[:, None]).astype(np.float)\n",
    "                feed_dict[train_labels[i]] = (np.arange(vocab_size + 1) == np.full(batch_size, vocab_size, dtype=np.int64)[:, None]).astype(np.float)\n",
    "            \n",
    "            # Train.\n",
    "            _, l, lr= session.run([optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "            mean_loss += l\n",
    "            \n",
    "        if step % summary_frequency == 0:\n",
    "            print('Step: %d, Learning Rate: %f, Average loss: %f' % (step, lr, mean_loss/(summary_frequency * max_seq_len)))\n",
    "            mean_loss = 0\n",
    "            \n",
    "            # Create five sample sequences and see how well the model reverses them.\n",
    "            print('       Input | Output')\n",
    "            for j in range(5):\n",
    "                reset_sample.run()\n",
    "                \n",
    "                # Create sequence.\n",
    "                seq_len = np.random.randint(max_seq_len) + 1\n",
    "                seq_in = np.append(np.random.randint(vocab_size, size=seq_len), np.full(max_seq_len - seq_len, vocab_size, dtype=np.int64))\n",
    "                \n",
    "                # Encode.\n",
    "                for i in range(max_seq_len):\n",
    "                    train_enc.run({sample_i: (np.arange(vocab_size + 1) == [[seq_in[i]]]).astype(np.float)})\n",
    "                    \n",
    "                # Prepare decoding.\n",
    "                seq_out = np.ndarray(0, dtype=np.int64)\n",
    "                feed = (np.arange(vocab_size + 1) == [[vocab_size]]).astype(np.float)\n",
    "                init_dec.run()\n",
    "                \n",
    "                # Decode until EOS recieved, or sequence length exceeds 2 * max_seq_len.\n",
    "                while len(seq_out) == 0 or (seq_out[-1] != vocab_size and len(seq_out) < 2 * max_seq_len):\n",
    "                    prediction = session.run([sample_prediction], {sample_i: feed})\n",
    "                    seq_out = np.append(seq_out, np.argmax(prediction))\n",
    "                    feed = (np.arange(vocab_size + 1) == [[seq_out[-1]]]).astype(np.float)\n",
    "                    \n",
    "                # Trim input/output, print results.\n",
    "                seq_in = seq_in[:seq_len]\n",
    "                seq_out = seq_out[:-1]\n",
    "                print('  ' * (max_seq_len - seq_len), seq_in, '|', seq_out)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
